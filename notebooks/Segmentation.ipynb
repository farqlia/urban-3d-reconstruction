{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:16.880142Z",
     "start_time": "2024-11-26T13:31:09.816727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchmetrics import Precision, Accuracy, Recall\n",
    "import open3d as o3d\n",
    "\n",
    "from urb3d.segmentation.train import classes_df\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "237d305689a9b199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:20.237413Z",
     "start_time": "2024-11-26T13:31:16.895768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_cloud = '../data/birmingham_blocks/birmingham_block_7_subsampled_train.ply'\n",
    "pt = PyntCloud.from_file(file_cloud)"
   ],
   "id": "7436f42f53afdffd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:20.568806Z",
     "start_time": "2024-11-26T13:31:20.553186Z"
    }
   },
   "cell_type": "code",
   "source": "pt",
   "id": "87ce8e5c578c26e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyntCloud\n",
       "588246 points with 4 scalar fields\n",
       "0 faces in mesh\n",
       "0 kdtrees\n",
       "0 voxelgrids\n",
       "Centroid: 914.6486206054688, 313.8023376464844, 10.899514198303223\n",
       "Other attributes:\n",
       "\t comments: <class 'list'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:23.868436Z",
     "start_time": "2024-11-26T13:31:20.584428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_cloud = '../data/birmingham_blocks/birmingham_block_7_subsampled_train.ply'\n",
    "pt = PyntCloud.from_file(file_cloud)\n",
    "classes_count = pd.DataFrame(pt.points['scalar_class']).value_counts().reset_index().set_index('scalar_class')\n",
    "classes_df = pd.DataFrame.from_dict({'scalar_class': np.arange(13), 'name': ['ground', 'vegetation', 'building', 'wall', 'bridge',\n",
    "    'parking', 'rail', 'traffic road', 'street furniture', 'car', 'footpath', 'bike', 'water']}).set_index('scalar_class')\n",
    "classes_df\n",
    "counts = classes_df.join(classes_count).fillna(0)\n",
    "sns.barplot(counts, y='name', x='count')\n",
    "plt.show()"
   ],
   "id": "93cfae3de71448d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:58:32.693393Z",
     "start_time": "2024-11-26T13:58:32.677771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "counts['weight'] = (1 / counts['count']).replace([np.inf, -np.inf], 1e-6)"
   ],
   "id": "ee37e6eb52d39397",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:02:13.033916Z",
     "start_time": "2024-11-26T14:02:13.002674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "counts['weight']"
   ],
   "id": "1ef76edee1634c64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalar_class\n",
       "0.0     0.000005\n",
       "1.0     0.000046\n",
       "2.0     0.000005\n",
       "3.0     0.000043\n",
       "4.0     0.000001\n",
       "5.0     0.000046\n",
       "6.0     0.000001\n",
       "7.0     0.000013\n",
       "8.0     0.000099\n",
       "9.0     0.000052\n",
       "10.0    0.000001\n",
       "11.0    0.000001\n",
       "12.0    0.000001\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:58:14.934095Z",
     "start_time": "2024-11-26T13:58:14.918473Z"
    }
   },
   "cell_type": "code",
   "source": "(1 / counts['count']).replace([np.inf, -np.inf], 1e-6)",
   "id": "7b6f9cf007b92fed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalar_class\n",
       "0.0     0.000005\n",
       "1.0     0.000046\n",
       "2.0     0.000005\n",
       "3.0     0.000043\n",
       "4.0     0.000001\n",
       "5.0     0.000046\n",
       "6.0     0.000001\n",
       "7.0     0.000013\n",
       "8.0     0.000099\n",
       "9.0     0.000052\n",
       "10.0    0.000001\n",
       "11.0    0.000001\n",
       "12.0    0.000001\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:23.899679Z",
     "start_time": "2024-11-26T13:31:23.884058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def min_max_standardize(coords):\n",
    "    x = (coords.loc[:, 'x'] - np.min(coords.loc[:, 'x'])) / (np.max(coords.loc[:, 'x']) - np.min(coords.loc[:, 'x']))\n",
    "    y = (coords.loc[:, 'y'] - np.min(coords.loc[:, 'y'])) / (np.max(coords.loc[:, 'y']) - np.min(coords.loc[:, 'y']))\n",
    "    z = (coords.loc[:, 'z'] - np.min(coords.loc[:, 'z'])) / (np.max(coords.loc[:, 'z']) - np.min(coords.loc[:, 'z']))\n",
    "\n",
    "    return x, y, z"
   ],
   "id": "c97eb3840c615952",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:23.931171Z",
     "start_time": "2024-11-26T13:31:23.915540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urb3d.segmentation.dataset import PointSampler\n",
    "from urb3d.segmentation.segmentor import segmentation_loss, PointNetSegmentor"
   ],
   "id": "856021f3b4c7967f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:23.962485Z",
     "start_time": "2024-11-26T13:31:23.946794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PointCloudSegmentationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, point_cloud_path, subsample_size=None, point_sampler=None, ds_size=None):\n",
    "        self.point_sampler = point_sampler\n",
    "        self.pt = PyntCloud.from_file(point_cloud_path)\n",
    "        self.ds_size = ds_size\n",
    "        self.subsample_size = subsample_size if subsample_size else len(self.pt.points)\n",
    "        x, y, z = min_max_standardize(self.pt.points[['x', 'y', 'z']])\n",
    "\n",
    "        self.pt.points['x_norm'] = x\n",
    "        self.pt.points['y_norm'] = y\n",
    "        self.pt.points['z_norm'] = z\n",
    "\n",
    "        self.label = 'class' if 'class' in self.pt.points.columns else 'scalar_class'\n",
    "\n",
    "    def __len__(self):\n",
    "        # How to set the number of batches?\n",
    "        return len(self.pt.points) // self.subsample_size if self.ds_size is None else self.ds_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For now, a point cloud is sampled each time this function is called (it doesn't depend on idx)\n",
    "        sampled_cloud = self.point_sampler(self.pt.points[['x_norm', 'y_norm', 'z_norm', self.label]])\n",
    "        sampled_points = torch.tensor(sampled_cloud[['x_norm', 'y_norm', 'z_norm']].values, device=device)\n",
    "        labels = torch.tensor(sampled_cloud[[self.label]].values, device=device, dtype=torch.long).flatten()\n",
    "        return sampled_points.T, labels"
   ],
   "id": "163847c48e6f5ecc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:24.409623Z",
     "start_time": "2024-11-26T13:31:24.009369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "segmentor = PointNetSegmentor(6)\n",
    "logits, m3x3, m64x64 = segmentor.forward(torch.randn(10, 1024, 3).transpose(2, 1))"
   ],
   "id": "ac5a364bfd5c6ebf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:24.854725Z",
     "start_time": "2024-11-26T13:31:24.839103Z"
    }
   },
   "cell_type": "code",
   "source": "m3x3",
   "id": "511f69631171aa66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9292, -0.1467,  0.3365],\n",
       "         [ 0.4216,  1.0073,  0.1374],\n",
       "         [-0.8984, -0.2184,  1.5467]],\n",
       "\n",
       "        [[ 0.9464,  0.3503, -0.5908],\n",
       "         [ 0.2367,  1.4359,  0.3926],\n",
       "         [ 0.3061, -0.0466,  1.3560]],\n",
       "\n",
       "        [[ 0.7965,  0.3181, -0.0070],\n",
       "         [ 0.8230,  1.4577,  0.0735],\n",
       "         [ 0.4060, -0.2196,  1.4472]],\n",
       "\n",
       "        [[ 0.5274,  0.4230, -0.0865],\n",
       "         [ 0.1373,  1.3695, -0.7927],\n",
       "         [-0.5281, -0.1415,  1.0303]],\n",
       "\n",
       "        [[ 1.0838,  0.4102,  0.0783],\n",
       "         [-0.8810,  0.6889,  0.2926],\n",
       "         [ 0.1957,  0.3453,  1.3724]],\n",
       "\n",
       "        [[ 0.7221,  0.5765, -0.4711],\n",
       "         [ 0.6713,  0.5174,  0.2551],\n",
       "         [-0.3215,  0.1081,  1.6596]],\n",
       "\n",
       "        [[ 1.0588, -0.0204,  0.5961],\n",
       "         [ 0.4065,  1.1886, -0.1885],\n",
       "         [-0.2573,  0.2655,  1.4298]],\n",
       "\n",
       "        [[ 0.9799,  0.2805, -0.2533],\n",
       "         [ 0.0141,  1.0167,  0.3186],\n",
       "         [-0.2326, -0.6788,  1.8239]],\n",
       "\n",
       "        [[ 1.0371, -0.0278, -0.2631],\n",
       "         [ 0.0857,  1.3163,  0.1891],\n",
       "         [ 0.4990,  0.5975,  1.6930]],\n",
       "\n",
       "        [[ 0.8297,  0.2488,  0.2886],\n",
       "         [ 0.2839,  1.1501,  0.2222],\n",
       "         [ 0.0483,  0.1527,  0.8788]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:26.484926Z",
     "start_time": "2024-11-26T13:31:26.469305Z"
    }
   },
   "cell_type": "code",
   "source": "logits.shape # (bs, # of classes, cld_s)",
   "id": "9b5094a34f192da2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:27.910795Z",
     "start_time": "2024-11-26T13:31:27.895172Z"
    }
   },
   "cell_type": "code",
   "source": "torch.argmax(logits, dim=1).shape",
   "id": "46c268e6e526d87b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:03:24.544535Z",
     "start_time": "2024-11-26T14:03:24.528909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "torch.Tensor(counts['weight'])"
   ],
   "id": "66772f2e2741ad4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5966e-06, 4.5756e-05, 5.0539e-06, 4.3354e-05, 1.0000e-06, 4.6019e-05,\n",
       "        1.0000e-06, 1.2998e-05, 9.9030e-05, 5.2233e-05, 1.0000e-06, 1.0000e-06,\n",
       "        1.0000e-06])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:03:38.919366Z",
     "start_time": "2024-11-26T14:03:38.872499Z"
    }
   },
   "cell_type": "code",
   "source": "F.cross_entropy(logits, torch.randint(low=0, high=12, size=(10, 1024)), weight=torch.Tensor(counts['weight']))",
   "id": "376802832afb7ab6",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all or no classes",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhigh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3103\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: weight tensor should be defined either for all or no classes"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T13:31:28.384953Z",
     "start_time": "2024-11-26T13:31:28.369331Z"
    }
   },
   "cell_type": "code",
   "source": "segmentation_loss(logits, torch.randint(low=0, high=6, size=(10, 1024)), m3x3, m64x64)\n",
   "id": "b8190a4ad0d32daa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1090, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:07:02.455953Z",
     "start_time": "2024-11-26T12:06:57.846262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subsample_size = 1024 # as in original paper but ofc can be different\n",
    "batch_size = 32\n",
    "\n",
    "training_dataset = PointCloudSegmentationDataset('../data/birmingham_blocks/birmingham_block_7_subsampled_train.ply',\n",
    "                                     subsample_size=subsample_size, point_sampler=PointSampler(subsample_size))\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = PointCloudSegmentationDataset('../data/birmingham_blocks/birmingham_block_7_subsampled_test.ply',\n",
    "                                     subsample_size=subsample_size, point_sampler=PointSampler(subsample_size))\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "model = PointNetSegmentor(classes=len(classes_df))"
   ],
   "id": "87c5f9a480fb3996",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:07:02.503181Z",
     "start_time": "2024-11-26T12:07:02.487352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PointCloudSegmentationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, point_cloud_path, subsample_size=None, point_sampler=None, ds_size=None):\n",
    "        self.point_sampler = point_sampler\n",
    "        self.pt: PyntCloud = PyntCloud.from_file(point_cloud_path)\n",
    "        # self.pt.points['scalar_class'] = -1  ## temp\n",
    "        self.ds_size = ds_size\n",
    "        self.subsample_size = subsample_size if subsample_size else len(self.pt.points)\n",
    "        x, y, z = min_max_standardize(self.pt.points[['x', 'y', 'z']])\n",
    "\n",
    "        self.pt.points['x_norm'] = x\n",
    "        self.pt.points['y_norm'] = y\n",
    "        self.pt.points['z_norm'] = z\n",
    "\n",
    "        self.label = 'class' if 'class' in self.pt.points.columns else 'scalar_class'\n",
    "\n",
    "    def get_pointcloud(self) -> PyntCloud:\n",
    "        return self.pt\n",
    "\n",
    "    def __len__(self):\n",
    "        # How to set the number of batches?\n",
    "        return len(self.pt.points) // self.subsample_size if self.ds_size is None else self.ds_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # For now, a point cloud is sampled each time this function is called (it doesn't depend on idx)\n",
    "        sampled_cloud = self.point_sampler(self.pt.points[['x_norm', 'y_norm', 'z_norm', self.label]])\n",
    "        sampled_points = torch.tensor(sampled_cloud[['x_norm', 'y_norm', 'z_norm']].values, device=device)\n",
    "        labels = torch.tensor(sampled_cloud[[self.label]].values, device=device, dtype=torch.long).flatten()\n",
    "        return sampled_points.T, labels"
   ],
   "id": "499783fb1360aeb0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T21:47:44.801133Z",
     "start_time": "2024-11-25T21:47:44.785509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = \"new_deal\"\n",
    "checkpoint_dir = Path(f\"../models/{experiment_name}\")\n",
    "logger = TensorBoardLogger(save_dir=f\"../models/{experiment_name}/tlogs\", name=experiment_name)\n",
    "\n",
    "max_epochs = 1"
   ],
   "id": "7944094e7e85de0e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T21:48:05.506837Z",
     "start_time": "2024-11-25T21:47:45.171256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir / 'best_results', filename='{epoch}-{val_loss:.2f}-{train_loss:.2f}',\n",
    "                                      monitor='val_loss', save_last=True)\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, fast_dev_run=False,\n",
    "                     default_root_dir=checkpoint_dir, callbacks=[checkpoint_callback], logger=logger)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "40185648d33c62b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\models\\new_deal\\best_results exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0  | precision | MulticlassPrecision | 0      | train\n",
      "1  | recall    | MulticlassRecall    | 0      | train\n",
      "2  | accuracy  | MulticlassAccuracy  | 0      | train\n",
      "3  | transform | Transform           | 2.8 M  | train\n",
      "4  | conv1     | Conv1d              | 557 K  | train\n",
      "5  | conv2     | Conv1d              | 131 K  | train\n",
      "6  | conv3     | Conv1d              | 32.9 K | train\n",
      "7  | conv4     | Conv1d              | 1.7 K  | train\n",
      "8  | bn1       | BatchNorm1d         | 1.0 K  | train\n",
      "9  | bn2       | BatchNorm1d         | 512    | train\n",
      "10 | bn3       | BatchNorm1d         | 256    | train\n",
      "11 | bn4       | BatchNorm1d         | 26     | train\n",
      "12 | dropout   | Dropout             | 0      | train\n",
      "-----------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "14.115    Total estimated model params size (MB)\n",
      "43        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9c886f443c841b7b21effe97575829d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "716937c2c95a4103bf05c3bfda737f80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2dccb15b5a949e2ad684887f85678e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:14:48.197486Z",
     "start_time": "2024-11-26T14:14:48.166244Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext tensorboard",
   "id": "b021fa6d24d42eb0",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:15:28.658474Z",
     "start_time": "2024-11-26T14:15:28.642852Z"
    }
   },
   "cell_type": "code",
   "source": "%reload_ext tensorboard",
   "id": "e3af2de89005e3b0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:15:48.174493Z",
     "start_time": "2024-11-26T14:15:40.959683Z"
    }
   },
   "cell_type": "code",
   "source": "%tensorboard --logdir ../models/pointnet_weighted\n",
   "id": "c2516230a5b616b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a01514c58e81146\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a01514c58e81146\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:18:44.798840Z",
     "start_time": "2024-11-26T14:18:44.783218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "2+2"
   ],
   "id": "f2c57c0ea8d55af1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T14:14:57.826640Z",
     "start_time": "2024-11-26T14:14:57.782780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "def timeit(tag, t):\n",
    "    print(\"{}: {}s\".format(tag, time() - t))\n",
    "    return time()\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)))\n",
    "        return new_points\n"
   ],
   "id": "3370bee821e99a23",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:54:27.984959Z",
     "start_time": "2024-11-26T12:54:27.953716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PointNetPP(pl.LightningModule):\n",
    "    def __init__(self, classes=13):\n",
    "        super().__init__()\n",
    "        self.precision = Precision(task=\"multiclass\", average='macro', num_classes=classes)\n",
    "        self.recall = Recall(task=\"multiclass\", average='macro', num_classes=classes)\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=classes)\n",
    "        self.sa1 = PointNetSetAbstraction(1024, 0.1, 32, 3 + 3, [32, 32, 64], False)\n",
    "        self.sa2 = PointNetSetAbstraction(256, 0.2, 32, 64 + 3, [64, 64, 128], False)\n",
    "        self.sa3 = PointNetSetAbstraction(64, 0.4, 32, 128 + 3, [128, 128, 256], False)\n",
    "        self.sa4 = PointNetSetAbstraction(16, 0.8, 32, 256 + 3, [256, 256, 512], False)\n",
    "        self.fp4 = PointNetFeaturePropagation(768, [256, 256])\n",
    "        self.fp3 = PointNetFeaturePropagation(384, [256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(320, [256, 128])\n",
    "        self.fp1 = PointNetFeaturePropagation(128, [128, 128, 128])\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv1d(128, classes, 1)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        l0_points = xyz\n",
    "        l0_xyz = xyz[:,:3,:]\n",
    "\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n",
    "\n",
    "        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points)\n",
    "\n",
    "        x = self.drop1(F.relu(self.bn1(self.conv1(l0_points))))\n",
    "        x = self.conv2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x, l4_points\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        seg_pred, trans_feat = self(x)\n",
    "        seg_pred = seg_pred.contiguous().view(-1, 13)\n",
    "\n",
    "        predictions = y.view(-1, 1)[:, 0]\n",
    "        loss = segmentation_loss(seg_pred, y, trans_feat)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('train_accuracy', self.accuracy(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('train_precision', self.precision(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('train_recall', self.recall(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        seg_pred, trans_feat = self(x)\n",
    "        seg_pred = seg_pred.contiguous().view(-1, 13)\n",
    "\n",
    "        predictions = y.view(-1, 1)[:, 0]\n",
    "        loss = segmentation_loss(seg_pred, y, trans_feat)\n",
    "        self.log('val_loss', loss, prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('val_accuracy', self.accuracy(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('val_precision', self.precision(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        self.log('val_recall', self.recall(predictions, y), prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        predictions, _= self(x)\n",
    "        return torch.argmax(predictions, dim=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def segmentation_loss(outputs, target, trans_feat):\n",
    "    total_loss = F.nll_loss(outputs, target)\n",
    "    return total_loss"
   ],
   "id": "34c88b33442c8087",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:56:16.060078Z",
     "start_time": "2024-11-26T12:56:14.969933Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch = train_loader.__iter__().__next__()[0]",
   "id": "d5519cfde0d7ea2c",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:54:19.524344Z",
     "start_time": "2024-11-26T12:54:18.940841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = PointNetPP(13)\n",
    "xyz = torch.rand(6, 9, 2048)\n",
    "res = model(xyz)"
   ],
   "id": "96af90b26d537f52",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 6, 1, 1], expected input[6, 12, 32, 1024] to have 6 channels, but got 12 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m PointNetPP(\u001B[38;5;241m13\u001B[39m)\n\u001B[0;32m      2\u001B[0m xyz \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m2048\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxyz\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[52], line 24\u001B[0m, in \u001B[0;36mPointNetPP.forward\u001B[1;34m(self, xyz)\u001B[0m\n\u001B[0;32m     21\u001B[0m l0_points \u001B[38;5;241m=\u001B[39m xyz\n\u001B[0;32m     22\u001B[0m l0_xyz \u001B[38;5;241m=\u001B[39m xyz[:,:\u001B[38;5;241m3\u001B[39m,:]\n\u001B[1;32m---> 24\u001B[0m l1_xyz, l1_points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msa1\u001B[49m\u001B[43m(\u001B[49m\u001B[43ml0_xyz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml0_points\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m l2_xyz, l2_points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msa2(l1_xyz, l1_points)\n\u001B[0;32m     26\u001B[0m l3_xyz, l3_points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msa3(l2_xyz, l2_points)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[17], line 198\u001B[0m, in \u001B[0;36mPointNetSetAbstraction.forward\u001B[1;34m(self, xyz, points)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_convs):\n\u001B[0;32m    197\u001B[0m     bn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_bns[i]\n\u001B[1;32m--> 198\u001B[0m     new_points \u001B[38;5;241m=\u001B[39m  F\u001B[38;5;241m.\u001B[39mrelu(bn(\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_points\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m    200\u001B[0m new_points \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(new_points, \u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    201\u001B[0m new_xyz \u001B[38;5;241m=\u001B[39m new_xyz\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Given groups=1, weight of size [32, 6, 1, 1], expected input[6, 12, 32, 1024] to have 6 channels, but got 12 channels instead"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:19:27.123294Z",
     "start_time": "2024-11-26T12:19:27.107672Z"
    }
   },
   "cell_type": "code",
   "source": "xyz.shape",
   "id": "f9697105486ed2c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 9, 2048])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:56:18.600191Z",
     "start_time": "2024-11-26T12:56:18.584569Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch.shape",
   "id": "b8012912de8071ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 1024])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:54:39.301495Z",
     "start_time": "2024-11-26T12:54:37.142848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.cuda()\n",
    "result = model(mini_batch)"
   ],
   "id": "40809f7f0bef71ee",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:54:43.974965Z",
     "start_time": "2024-11-26T12:54:43.959343Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch[1].shape",
   "id": "b471dfa6caf9d4a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:55:02.717915Z",
     "start_time": "2024-11-26T12:55:02.702290Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch[1].view(-1, 1)[:, 0].shape",
   "id": "1db12e45bf4ba144",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:52:43.973386Z",
     "start_time": "2024-11-26T12:52:43.957718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sa_test = PointNetSetAbstraction(1024, 0.1, 32, 6, [32, 32, 64], False)\n",
    "sa_test.cuda()"
   ],
   "id": "e36066a941a3d792",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointNetSetAbstraction(\n",
       "  (mlp_convs): ModuleList(\n",
       "    (0): Conv2d(6, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (mlp_bns): ModuleList(\n",
       "    (0-1): 2 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:19:16.408521Z",
     "start_time": "2024-11-26T12:19:15.224883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "l0_points = mini_batch\n",
    "l0_xyz = mini_batch\n",
    "\n",
    "l1_xyz, l1_points = (sa_test(l0_xyz, l0_points))"
   ],
   "id": "7d102bf37732be50",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:55:56.950265Z",
     "start_time": "2024-11-26T12:55:56.934643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "mini_batch[1].shape"
   ],
   "id": "e406725fa45ec8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:55:29.684523Z",
     "start_time": "2024-11-26T12:55:29.638711Z"
    }
   },
   "cell_type": "code",
   "source": "F.nll_loss(mini_batch,mini_batch[1] )",
   "id": "542faefbffe2ef6",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nll_loss_nd(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[89], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2778\u001B[0m, in \u001B[0;36mnll_loss\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[0;32m   2776\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2777\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 2778\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:33:24.848877Z",
     "start_time": "2024-11-26T12:33:24.833254Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch[1].shape",
   "id": "62f44f40dcecfa34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T23:41:12.496998Z",
     "start_time": "2024-11-25T23:41:12.471563Z"
    }
   },
   "cell_type": "code",
   "source": "model = PointNetPP(13)",
   "id": "3ccb4494bc684d3f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:19:47.783634Z",
     "start_time": "2024-11-26T12:19:45.700122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = \"pointnetpp\"\n",
    "checkpoint_dir = Path(f\"../models/{experiment_name}\")\n",
    "logger = TensorBoardLogger(save_dir=f\"../models/{experiment_name}/tlogs\", name=experiment_name)\n",
    "\n",
    "max_epochs = 1\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir / 'best_results',\n",
    "                                                   filename='{epoch}-{val_loss:.2f}-{train_loss:.2f}',\n",
    "                                                   monitor='val_loss', save_last=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, fast_dev_run=False,\n",
    "                     default_root_dir=checkpoint_dir, callbacks=[checkpoint_callback], logger=logger)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "2e40d35fc85c0491",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                       | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0  | precision | MulticlassPrecision        | 0      | train\n",
      "1  | recall    | MulticlassRecall           | 0      | train\n",
      "2  | accuracy  | MulticlassAccuracy         | 0      | train\n",
      "3  | sa1       | PointNetSetAbstraction     | 3.6 K  | train\n",
      "4  | sa2       | PointNetSetAbstraction     | 17.3 K | train\n",
      "5  | sa3       | PointNetSetAbstraction     | 67.5 K | train\n",
      "6  | sa4       | PointNetSetAbstraction     | 265 K  | train\n",
      "7  | fp4       | PointNetFeaturePropagation | 263 K  | train\n",
      "8  | fp3       | PointNetFeaturePropagation | 165 K  | train\n",
      "9  | fp2       | PointNetFeaturePropagation | 115 K  | train\n",
      "10 | fp1       | PointNetFeaturePropagation | 50.3 K | train\n",
      "11 | conv1     | Conv1d                     | 16.5 K | train\n",
      "12 | bn1       | BatchNorm1d                | 256    | train\n",
      "13 | drop1     | Dropout                    | 0      | train\n",
      "14 | conv2     | Conv1d                     | 1.7 K  | train\n",
      "------------------------------------------------------------------\n",
      "968 K     Trainable params\n",
      "0         Non-trainable params\n",
      "968 K     Total params\n",
      "3.872     Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15620e8c470c4e509aff3e64b5046e52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasia\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32768) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 13\u001B[0m\n\u001B[0;32m      6\u001B[0m checkpoint_callback \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(dirpath\u001B[38;5;241m=\u001B[39mcheckpoint_dir \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_results\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      7\u001B[0m                                                    filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{epoch}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{val_loss:.2f}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{train_loss:.2f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      8\u001B[0m                                                    monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, save_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39mmax_epochs, fast_dev_run\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     11\u001B[0m                      default_root_dir\u001B[38;5;241m=\u001B[39mcheckpoint_dir, callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback], logger\u001B[38;5;241m=\u001B[39mlogger)\n\u001B[1;32m---> 13\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1023\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1025\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1049\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1052\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[0;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[0;32m    395\u001B[0m )\n\u001B[1;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:411\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[52], line 60\u001B[0m, in \u001B[0;36mPointNetPP.validation_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     57\u001B[0m seg_pred \u001B[38;5;241m=\u001B[39m seg_pred\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m13\u001B[39m)\n\u001B[0;32m     59\u001B[0m predictions \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)[:, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 60\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43msegmentation_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseg_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans_feat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, sync_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccuracy(predictions, y), prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, sync_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[52], line 78\u001B[0m, in \u001B[0;36msegmentation_loss\u001B[1;34m(outputs, target, trans_feat)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msegmentation_loss\u001B[39m(outputs, target, trans_feat):\n\u001B[1;32m---> 78\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m total_loss\n",
      "File \u001B[1;32m~\\PycharmProjects\\urban-3d-reconstruction\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2778\u001B[0m, in \u001B[0;36mnll_loss\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[0;32m   2776\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2777\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 2778\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected input batch_size (32768) to match target batch_size (32)."
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T12:06:33.845696Z",
     "start_time": "2024-11-26T12:06:33.077475Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "29ade612c655e3a9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PointNetSetAbstraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m sa_test \u001B[38;5;241m=\u001B[39m \u001B[43mPointNetSetAbstraction\u001B[49m(\u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m9\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m3\u001B[39m, [\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m64\u001B[39m], \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'PointNetSetAbstraction' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a91dc42c9741e3d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
